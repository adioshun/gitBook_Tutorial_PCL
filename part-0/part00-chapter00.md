---
description: '20210112 : 초안 작성'
---

# chapter00 : 3D 영상처리

## 0. 3D영상처리 

우리의 오감 중에서 가장 중요한 요소를 뽑으라면 대부분의 사람들은 눈을 선택할 것입니다. 그만큼 세상을 인지하는 데 있어 시각 즉, 영상은 많은 정보를 포함하고 있습니다. 영상정보에서 의미 있는 정보를 추출하는 연구는 오래 전부터 이뤄져 왔습니다. 이는 보통 영상신호처리, 이미지처리, 컴퓨터 비전, 로봇비전, 패턴인식 등의 연구 분야에서 진행돼 왔습니다. \[그림 1.1\]은 영상처리와 관련된 연구 분야를 도표화한 것입니다.

![&#xADF8;&#xB9BC; 1.1 3D &#xC601;&#xC0C1;&#xCC98;&#xB9AC; &#xC5F0;&#xAD6C; &#xBD84;&#xC57C;](../.gitbook/assets/image.png)

본문에서 다루고자 하는 3D 영상처리는 최종적으로 로봇 비전과 연관이 있으며 하드웨어인 카메라나 거리센서의 이미지/신호처리 처리를 통해 영상을 획득하고 패턴인식 등의 머신러닝 알고리즘을 이용해 공간과 사물을 인식하는 것을 목표로 하고 있습니다. \[그림 1.1\]에서 회색으로 표시된 부분 입니다. 각 용어들에 대한 자세한 설명은 아래와 같습니다\(Alex Owen-Hill, 2016\). 

**신호 처리\(Signal Processing\):** 신호처리란 신호에서 잡음을 제거하거나 정보를 추출해 이후 후속작업에서 활용할 수 있게 다듬는 작업을 의미합니다. 신호는 아날로그\(Analog electrical signals\) 또는 디지털 신호\(Digital electronic signals\) 로 표현되며, 음향, 전자기파, 영상 또는 센서 출력 값 등 다양한 측정값을 표현할 수 있습니다. 본문에서는 영상 또는 센서 출력 값을 주로 다루고 있습니다. 

**이미지 처리\(Image Processing\) vs 컴퓨터 비전\(Computer Vision\):** 컴퓨터 비전과 이미지 처리는 같은 레벨이지만 목적은 다릅니다. 이미지 처리 기술은 주로 이미지 질을 향상시키거나 포맷\(e.g. 히스토그램\)을 변경해 이후 작업에 활용하는데 목적을 두고 있습니다. 반면, 컴퓨터 비전은 이미지에서 정보를 추출해 이해하는데 목적을 두고 있습니다. 즉, 이미지 처리를 통해서 컬러 이미지를 흑백으로 변경하고 컴퓨터 비전을 통해서 이미지상 물체를 탐지할 수 있습니다. 

**패턴인식\(Pattern Recognition\) and 머신러닝\(Machine Learning\):** 패턴인식 또는 머신러닝은 데이터에서 특정한 패턴을 탐지하는데 목적을 두고 있습니다. 데이터는 엑셀파일과 같은 정형데이터 일수도 있고, 이미지나 센서와 같은 비정형 데이터 일수도 있습니다. 따라서 컴퓨터 비전에 국한되지 않는 상위 계층 분야에 속합니다. 그러나 모든 컴퓨터 비전분야에서 머신러닝을 필요로 하지는 않으며 이미지나 센서 신호가 아닌 데이터에서도 머신러닝을 사용할 수 있습니다. 

**머신 비전\(Machine Vision\):** 지금까지 살펴본 것들이 과학적 영역\(Scientific domains\)이라면 머신비전은 공학적 영역\(Engineering domain\)에 속합니다. 머신비전은 위의 기술들을 이용해 정밀검사, 공정제어 등의 산업적 활용하는 응용단을 의미합니다. 

**로봇 비전\(Robot Vision\):** 로봇 비전은 일부 머신 비전의 기능을 사용하기도 합니다. 하지만 모든 머신 비전이 로봇 비전은 아닙니다. 예를 들어 정밀검사는 센서를 사용해 물체의 문제점을 찾는 것으로 로봇 하고는 관련이 없습니다. 또한, 로봇비전은 과학적 영역과 공학적 영역에 모두 포함돼 있습니다. 순수한 컴퓨터 비전 연구 분야와 달리 로봇 비전은 개발 시 로봇의 동역학, 물리적 변화 등을 고려해야 합니다.

\[표 1.1\]은 지금까지 이야기한 용어들의 입력 및 출력을 표로 정리한 것입니다.

![&#xD45C; 1.1 3D &#xC601;&#xC0C1;&#xCC98;&#xB9AC; &#xC5F0;&#xAD6C; &#xBD84;&#xC57C;&#xBCC4; &#xC785;&#xB825;&#xACFC; &#xCD9C;&#xB825; &#xACB0;&#xACFC;&#xBB3C; &#xC815;&#xB9AC; ](https://user-images.githubusercontent.com/17797922/104254047-5368eb80-54b9-11eb-8ce1-bd2518dd873a.png)

본문 다루는 분야는 이미지 처리를 통해 센서에서 들어오는 데이터를 분석이 가능한 형태로 가공하는 단계\(2장\), 컴퓨터 비전을 통해 분석에 효율적인 특징들을 추출하는 단계\(3장\), 패턴인식/머신러닝을 활용해 추출된 특징들을 기반으로 물체를 인식하는 머신 비전/로봇 비전 단계\(3장\) 까지를 다루고 있습니다. 각 단계들은 실습을 통해 직접 입력결과와 출력 결과를 시각적으로 확인할 수 있게 하였습니다. 시작에 앞서 사용자에게 익숙한 2D 이미지 데이터와 3D 센서 데이터의 특징 및 분석 방법을 비교해 서로의 차이점에 대한 이해도를 높이겠습니다.

### 0.1 2D/3D 영상의 역사와 장비 

최초의 2D 영상은 1826년 니에프스\(Joseph Nicephore Niepce\)가 8시간에 걸쳐 찍은 아날로그 사진입니다. 그는 이 사진을 태양광선으로 그리는 그림이라는 뜻의 헬리오그래피\(heliography\)라고 불렀습니다. 하지만 노출 시간이 너무 길기 쉽게 상업화되지는 않았습니다. 최초의 디지털 카메라는 1957년 미국국립표준기술연구소\(NIST\)에서 디지털 스캐너로 입력한 이미지입니다. 이후 1975년 코닥 직원인 스티븐 새슨이 최초의 디지털 카메라를 개발하였습니다. CCD이미지 센서를 사용해 1메가 픽셀\(100 x 100\) 해상도의 흑백 사진을 촬영할 수 있습니다. 오늘날에는 휴대폰에서 1,200만 픽셀의 이미지 촬영이 가능합니다. \[그림 1.2\]는 최초의 2D 영상 데이터와 장비입니다.

![&#xADF8;&#xB9BC; 1.2 2D &#xC601;&#xC0C1;&#xC758; &#xC5ED;&#xC0AC;&#xC640; &#xC7A5;&#xBE44;](https://user-images.githubusercontent.com/17797922/104254263-cd997000-54b9-11eb-8ff6-065ee0cd6467.png)

3D 영상은 센서에 따라 크게 수동 방식과 능동 방식으로 수집할 수 있습니다. 수동 방식은 사람의 눈과 같이 두대의 카메라를 이용해 수집하는 방식으로 1838년 찰스 휘트스톤\(Charles Wheatstone\)의 양안 시차 발견과 함께 시작됐습니다. 이후 1849년 다비드 브루스터\(David Brewster\)가 프리즘식 입체경을 고안하면서 널리 활성화됐습니다. 최근 영화관에서 상영되는 3D 영상들도 이런 양안 시차를 기반으로 한 것입니다. \[그림 1.3\]은 수동 방식의 3D 영상 데이터 수집장비와 데이터입니다.



![&#xADF8;&#xB9BC; 1.3 &#xC218;&#xB3D9; &#xBC29;&#xC2DD; 3D &#xC601;&#xC0C1; &#xC7A5;&#xBE44;&#xC640; &#xACB0;&#xACFC;&#xBB3C;](https://user-images.githubusercontent.com/17797922/104254343-f4f03d00-54b9-11eb-9a88-54a26d3db071.png)

능동방식은 적외선이나 초음파, 빛 등을 이용해 거리를 측정하는 방식입니다. MS가 2009년에 공개한 키넥트 센서는 대표적인 적외선 기반 3D센서입니다. 키넥트는 카메라 센서로 이미지\(RGB\)정보를 획득하고, IR센서로 깊이\(Depth\) 정보를 동시에 수집할 수 있어 RGB-D센서라도 부릅니다. 다른 센서인 라이다\(Light Detection and Ranging, LIDAR\)는 빛을 이용해 3D위치 정보\(x,y,z\)를 수집합니다. RGB-D센서 대비 카메라가 없어 색상 정보 획득은 어렵지만 긴 측정 범위와 야외 환경에서도 안전적인 데이터 수집이 가능해 자율주행차량에서 많이 사용되고 있습니다. \[그림 1.4\]는 능동 방식의 3D 영상 데이터 수집장비와 데이터입니다.



![&#xADF8;&#xB9BC; 1.4 &#xB2A5;&#xB3D9; &#xBC29;&#xC2DD; 3D &#xC601;&#xC0C1; &#xC7A5;&#xBE44;&#xC640; &#xACB0;&#xACFC;&#xBB3C;](https://user-images.githubusercontent.com/17797922/104254482-41d41380-54ba-11eb-9a1d-d6909c213442.png)

수동 방식은 능동 방식에 비해 시스템 구성이 간편하나, 센싱 환경에 민감해 안정적인 데이터를 수집할 수 없는 단점이 있습니다. 능동 방식은 수동 방식에 비해 주변 환경 변화에 강인하고 풍부한 거리 데이터를 수집할 수 있어 로봇이나 자율주행차량과 같은 안전적인 물체 센싱이 필요한 환경에 많이 적용되고 있습니다. 최근에는 2D 영상과 3D 능동/수동 방식의 데이터를 모두 활용해 각 센서의 장단점을 보완해 안전성을 높이는 방식이 선호되고 있습니다. \[표 1.2\]는 3D 영상을 수집하는 수동 방식과 능동 방식을 비교한 표입니다.

![&#xD45C; 1.2 3D &#xC601;&#xC0C1; &#xC218;&#xC9D1; &#xBC29;&#xC2DD; &#xBE44;&#xAD50; ](https://user-images.githubusercontent.com/17797922/104254538-67f9b380-54ba-11eb-9dec-cdcc7a7a9b4f.png)

본문에서는 능동 방식을 중심으로 2장에서는 RGB-D센서 데이터를 이용한 로봇의 물체 인식과, 3장의 LIDAR센서 데이터를 이용한 차량 탐지를 통해 좀더 자세한 내용을 다루겠습니다.

