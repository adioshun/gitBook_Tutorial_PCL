{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습용 데이터 셋 생성 \n",
    "\n",
    "Color_histogram + Normal_histogram + Lable = training_set.sav 생성을 목표로 함\n",
    "\n",
    "- [capture_features.py](https://github.com/mkhuthir/RoboND-Perception-Project/blob/master/src/sensor_stick/scripts/capture_features.py)\n",
    "- [features.py](https://github.com/mkhuthir/RoboND-Perception-Project/blob/master/src/sensor_stick/src/sensor_stick/features.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcl_helper import *\n",
    "#https://github.com/mkhuthir/RoboND-Perception-Project/blob/master/src/sensor_stick/scripts/pcl_helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors\n",
    "\n",
    "def rgb_to_hsv(rgb_list):\n",
    "    rgb_normalized = [1.0*rgb_list[0]/255, 1.0*rgb_list[1]/255, 1.0*rgb_list[2]/255]\n",
    "    hsv_normalized = matplotlib.colors.rgb_to_hsv([[rgb_normalized]])[0][0]\n",
    "    return hsv_normalized\n",
    "\n",
    "def compute_color_histograms_PCD(cloud, using_hsv=False):\n",
    "\n",
    "    # Compute histograms for the clusters\n",
    "    point_colors_list = []\n",
    "\n",
    "    \"\"\"\n",
    "    # Step through each point in the point cloud for ROS msg\n",
    "    for point in pc2.read_points(cloud, skip_nans=True): \n",
    "        rgb_list = float_to_rgb(point[3])\n",
    "        if using_hsv:\n",
    "            point_colors_list.append(rgb_to_hsv(rgb_list) * 255)\n",
    "        else:\n",
    "            point_colors_list.append(rgb_list)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step through each point in the point cloud for PCD\n",
    "    for point in cloud[:,3]: # for PCD file\n",
    "        rgb_list = float_to_rgb(point)\n",
    "        if using_hsv:\n",
    "            point_colors_list.append(rgb_to_hsv(rgb_list) * 255)\n",
    "        else:\n",
    "            point_colors_list.append(rgb_list)\n",
    "\n",
    "    # Populate lists with color values\n",
    "    channel_1_vals = []\n",
    "    channel_2_vals = []\n",
    "    channel_3_vals = []\n",
    "\n",
    "    for color in point_colors_list:\n",
    "        channel_1_vals.append(color[0])\n",
    "        channel_2_vals.append(color[1])\n",
    "        channel_3_vals.append(color[2])\n",
    "    \n",
    "    # Compute histograms\n",
    "    nbins=32\n",
    "    bins_range=(0, 256)\n",
    "        \n",
    "    # Compute the histogram of the channels separately\n",
    "    channel_1_hist = np.histogram(channel_1_vals, bins=nbins, range=bins_range)\n",
    "    channel_2_hist = np.histogram(channel_2_vals, bins=nbins, range=bins_range)\n",
    "    channel_3_hist = np.histogram(channel_3_vals, bins=nbins, range=bins_range)\n",
    "    \n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel_1_hist[0], channel_2_hist[0], channel_1_hist[0])).astype(np.float64)\n",
    "    \n",
    "    # Normalize the result\n",
    "    normed_features = hist_features / np.sum(hist_features)\n",
    "\n",
    "    # Generate random features for demo mode.  \n",
    "    # Replace normed_features with your feature vector\n",
    "    #normed_features = np.random.random(96) \n",
    "\n",
    "    # Return the feature vector\n",
    "    return normed_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_normals(cloud_path):\n",
    "    \"\"\"\n",
    "    The actual *compute* call from the NormalEstimation class does nothing internally but:\n",
    "    for each point p in cloud P\n",
    "        1. get the nearest neighbors of p\n",
    "        2. compute the surface normal n of p\n",
    "        3. check if n is consistently oriented towards the viewpoint and flip otherwise\n",
    "\n",
    "    # normals: pcl._pcl.PointCloud_Normal,size: 26475\n",
    "    # cloud: pcl._pcl.PointCloud\n",
    "    \"\"\"\n",
    "    cloud = pcl.load(cloud_path)\n",
    "    \n",
    "    feature = cloud.make_NormalEstimation()\n",
    "    #feature.set_RadiusSearch(0.1) #Use all neighbors in a sphere of radius 3cm\n",
    "    \n",
    "    feature.set_KSearch(3)\n",
    "    normals = feature.compute()\n",
    "    \n",
    "    return normals\n",
    "\n",
    "def compute_normal_histograms(normal_cloud, nbins=32, nrange=(-1,1)):\n",
    "    '''\n",
    "    Computes and bins the point-cloud data using the objects distribution of surface normals.\n",
    "    :param: normal_cloud, point cloud containing the filtered clusters.\n",
    "    :param: nbins,number of bins that data will be pooled into.\n",
    "    :param: nrange, value range of the data to be pooled.\n",
    "    :return: the normalised histogram of surface normals\n",
    "    '''\n",
    "    norm_x_vals = []\n",
    "    norm_y_vals = []\n",
    "    norm_z_vals = []\n",
    "\n",
    "    for I in range(0,normal_cloud.size):\n",
    "        norm_x_vals.append(normal_cloud[I][0])\n",
    "        norm_y_vals.append(normal_cloud[I][1])\n",
    "        norm_z_vals.append(normal_cloud[I][2])\n",
    "\n",
    "    # Compute histograms of normal values (just like with color)\n",
    "    norm_x_hist = np.histogram(norm_x_vals, bins=nbins, range=nrange)\n",
    "    norm_y_hist = np.histogram(norm_y_vals, bins=nbins, range=nrange)\n",
    "    norm_z_hist = np.histogram(norm_z_vals, bins=nbins, range=nrange) \n",
    "\n",
    "    # Concatenate and normalize the histograms\n",
    "    hist_features = np.concatenate((norm_x_hist[0], norm_y_hist[0], norm_z_hist[0])).astype(np.float64)\n",
    "    normed_features = hist_features / np.sum(hist_features)\n",
    "\n",
    "    return normed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_path = \"tabletop.pcd\"\n",
    "cloud = pcl.load_XYZRGB(cloud_path)\n",
    "cloud_arr = cloud.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Color Histogram for the spawned model\n",
    "# Enable using_hsv for better results\n",
    "c_hists = compute_color_histograms_PCD(cloud_arr, using_hsv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/numpy/lib/function_base.py:748: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= mn)\n",
      "/usr/lib/python2.7/dist-packages/numpy/lib/function_base.py:749: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= mx)\n"
     ]
    }
   ],
   "source": [
    "# Generate normals and notmal histograms for the spawned model\n",
    "normals = get_normals(cloud_path)\n",
    "n_hists = compute_normal_histograms(normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature by concatenate of color and normals.\n",
    "feature = np.concatenate((c_hists, n_hists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행 에러 : [소스 코드](https://github.com/mkhuthir/RoboND-Perception-Project/blob/master/src/sensor_stick/scripts/capture_features.py#L52) 확인 필요 \n",
    "\n",
    "```python \n",
    "models = [\\\n",
    "    #'arm_part',\n",
    "    #'beer',\n",
    "    'biscuits',\n",
    "    'book',\n",
    "    #'bowl',\n",
    "    #'create',\n",
    "    #'disk_part',\n",
    "    'eraser',\n",
    "    'glue',\n",
    "    #'hammer',\n",
    "    #'plastic_cup',\n",
    "    'snacks',\n",
    "    'soap',\n",
    "    'soap2',\n",
    "    #'soda_can',\n",
    "    'sticky_notes'\n",
    "    ]\n",
    "\n",
    "# add feature to list\n",
    "labeled_features.append([feature, model_name])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(labeled_features open('training_set.sav', 'wb'))\n",
    "#https://raw.githubusercontent.com/dexter800/RoboND-Perception-Project/master/training_set.sav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [train_svm.py](https://github.com/mkhuthir/RoboND-Perception-Project/blob/master/src/sensor_stick/scripts/train_svm.py)\n",
    "- training_set.sav 활용 model.sav 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import cross_validate #from sklearn import cross_validation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, '{0:.2f}'.format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from disk\n",
    "training_set = pickle.load(open('training_set.sav', 'rb'))\n",
    "\n",
    "# Format the features and labels for use with scikit learn\n",
    "feature_list = []\n",
    "label_list = []\n",
    "\n",
    "for item in training_set:\n",
    "    if np.isnan(item[0]).sum() < 1:\n",
    "        feature_list.append(item[0])\n",
    "        label_list.append(item[1])\n",
    "\n",
    "print('Features in Training Set: {}'.format(len(training_set)))\n",
    "print('Invalid Features in Training set: {}'.format(len(training_set)-len(feature_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(feature_list)\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "X_train = X_scaler.transform(X)\n",
    "y_train = np.array(label_list)\n",
    "\n",
    "# Convert label strings to numerical encoding\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> sklearn 업데이트로 코드 수정 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier\n",
    "# Options linear, rbf, sigmoid, poly.\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = cross_validation.KFold(len(X_train),\n",
    "                            n_folds=5,\n",
    "                            shuffle=True,\n",
    "                            random_state=1)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_validation.cross_val_score(cv=kf,\n",
    "                                         estimator=clf,\n",
    "                                         X=X_train, \n",
    "                                         y=y_train,\n",
    "                                         scoring='accuracy'\n",
    "                                        )\n",
    "print('Scores: ' + str(scores))\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather predictions\n",
    "predictions = cross_validation.cross_val_predict(cv=kf,\n",
    "                                          estimator=clf,\n",
    "                                          X=X_train, \n",
    "                                          y=y_train\n",
    "                                         )\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(y_train, predictions)\n",
    "print('accuracy score: '+str(accuracy_score))\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_train, predictions)\n",
    "\n",
    "class_names = encoder.classes_.tolist()\n",
    "\n",
    "\n",
    "#Train the classifier\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "model = {'classifier': clf, 'classes': encoder.classes_, 'scaler': X_scaler}\n",
    "\n",
    "# Save classifier to disk\n",
    "pickle.dump(model, open('model.sav', 'wb'))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confusion_matrix, classes=encoder.classes_,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confusion_matrix, classes=encoder.classes_, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [예측 ](https://github.com/mkhuthir/RoboND-Perception-Project/blob/master/src/sensor_stick/scripts/object_recognition.py#L142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('model.sav', 'rb'))\n",
    "#https://raw.githubusercontent.com/dexter800/RoboND-Perception-Project/master/model.sav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model['classifier']\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = model['classes']\n",
    "scaler = model['scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_path = \"table_scene_inliers_0.pcd\"\n",
    "cloud = pcl.load_XYZRGB(cloud_path)\n",
    "cloud_arr = cloud.to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract histogram features\n",
    "################################\n",
    "# Generate Color hist\n",
    "c_hists = compute_color_histograms_PCD(cloud_arr, using_hsv=True)\n",
    "\n",
    "# Generate normals and notmal histograms for the spawned model\n",
    "################################\n",
    "normals = get_normals(cloud_path)\n",
    "n_hists = compute_normal_histograms(normals)\n",
    "\n",
    "# Generate feature by concatenate of color and normals.\n",
    "################################\n",
    "feature = np.concatenate((c_hists, n_hists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_objects = []\n",
    "# Make the prediction, retrieve the label for the result\n",
    "# and add it to detected_objects_labels list\n",
    "################################\n",
    "prediction = clf.predict(scaler.transform(feature.reshape(1,-1)))\n",
    "label = encoder.inverse_transform(prediction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soap2'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
